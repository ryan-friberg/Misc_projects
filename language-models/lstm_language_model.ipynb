{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWCOlx1vSmf"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zyBV10nMJIrO"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/a3_data/lm_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j2IKVqkxzN1L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import random\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7asRBEtT102s"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 100\n",
        "WORD_EMBED_DIM = 200\n",
        "HID_EMBED_DIM = 200\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "LOG_INTERVAL = 100\n",
        "EPOCHS = 20\n",
        "BPTT = 50 # sequence length\n",
        "CLIP = 0.25\n",
        "TIED = False\n",
        "SAVE_BEST = os.path.join(DATA_DIR, 'model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_45k6t7hzgsA"
      },
      "source": [
        "## Build vocabulary and convert text in corpus to lists of word index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr3dkhUg2fiG",
        "outputId": "5b12d8aa-7bc8-49f5-a017-5179d68b08b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2099444\n",
            "218808\n",
            "246993\n",
            "28913\n"
          ]
        }
      ],
      "source": [
        "class WordDict(object):\n",
        "    def __init__(self):\n",
        "        # mapping between word type to its index\n",
        "        self.word2idx = {}\n",
        "        # mapping between index to word type\n",
        "        self.idx2word = {}\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word[self.word2idx[word]] = word\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.train_file = os.path.join(path, 'train.txt')\n",
        "        self.valid_file = os.path.join(path, 'valid.txt')\n",
        "        self.test_file = os.path.join(path, 'test.txt')\n",
        "\n",
        "        self.dictionary = WordDict()\n",
        "\n",
        "        self.train = self.tokenize(self.train_file)\n",
        "        self.valid = self.tokenize(self.valid_file)\n",
        "        self.test = self.tokenize(self.test_file)\n",
        "\n",
        "    def tokenize(self, filename):\n",
        "        corpus = open(filename).readlines()\n",
        "        ids = []\n",
        "        for line in corpus:\n",
        "          line = line.strip()\n",
        "          if (line != '\\n') and (line != ''):\n",
        "              tokens = line.lower().rstrip().split()\n",
        "              tokens.insert(0, '<sos>')\n",
        "              tokens.append('<eos>')\n",
        "              for token in tokens:\n",
        "                  ids.append(self.dictionary.add_word(token))\n",
        "        return ids\n",
        "\n",
        "corpus = Corpus(DATA_DIR)\n",
        "print(len(corpus.train))\n",
        "print(len(corpus.valid))\n",
        "print(len(corpus.test))\n",
        "print(len(corpus.dictionary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6of33pFT94dM",
        "outputId": "7c65ed75-6370-4c66-bb20-b53905d5180b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([20994, 100])\n",
            "torch.Size([2188, 100])\n",
            "torch.Size([2469, 100])\n"
          ]
        }
      ],
      "source": [
        "def batchify(ids, batch_size):\n",
        "    num_batches = len(ids) // batch_size\n",
        "    data = []\n",
        "    for i in range(num_batches):\n",
        "        batch = []\n",
        "        for j in range(batch_size):\n",
        "            val = ids[i + (j*num_batches)]\n",
        "            batch.append(val)\n",
        "        data.append(batch)\n",
        "    return torch.LongTensor(data)\n",
        "\n",
        "train_data = batchify(corpus.train, TRAIN_BATCH_SIZE)\n",
        "val_data = batchify(corpus.valid, TEST_BATCH_SIZE)\n",
        "test_data = batchify(corpus.test, TEST_BATCH_SIZE)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qhtQEbhR_hNT",
        "outputId": "864c02b7-d3a2-4512-b4f2-b8576e0bfebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    0,   701,    10,  ...,    18, 28809,   272],\n",
            "        [    1,  1791,    14,  ...,   438,  8623, 20553],\n",
            "        [    2,   130,   119,  ...,   984,    18,   300],\n",
            "        ...,\n",
            "        [   35,    17,  5419,  ...,  5099,    16,    14],\n",
            "        [   36,   346,    62,  ...,    14,     5,  1625],\n",
            "        [   37,  3544,    38,  ...,  7773,     0,  1654]])\n",
            "tensor([    1,  1791,    14,  ..., 17113,     1,  5407])\n"
          ]
        }
      ],
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = BPTT if i + BPTT < len(source) else len(source) - 1 - i\n",
        "    data = source[i:i+seq_len,:]\n",
        "    target = torch.flatten(source[i+1:i+seq_len+1,:])\n",
        "    return data, target\n",
        "\n",
        "data, targets = get_batch(train_data, 0)\n",
        "print(data)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6C8QNsUL9i1S"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, word_embedding_size, nhid, nlayers, dropout=0.5, tied_weights=False):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.nhid = nhid # hidden dimension of LSTM\n",
        "        self.nlayers = nlayers # number of LSTM layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.encoder = torch.nn.Embedding(vocab_size, word_embedding_size)\n",
        "        self.lstm = nn.LSTM(input_size=word_embedding_size, hidden_size=self.nhid, num_layers=self.nlayers, batch_first=False)\n",
        "        self.output = nn.Linear(self.nhid, self.vocab_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        For example:\n",
        "        # initrange = 0.1\n",
        "        # nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        This is not all that you need!\n",
        "        \"\"\"\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -1*initrange, initrange)\n",
        "        for ls in self.lstm._all_weights:\n",
        "          for param in ls:\n",
        "            if param in self.lstm.__dict__:\n",
        "              nn.init.uniform_(self.lstm.__dict__[param], -1*initrange, initrange)\n",
        "        nn.init.uniform_(self.output.weight, -1*initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        # Parameters\n",
        "        input: input embedding\n",
        "        hidden: hidden states in LSTM\n",
        "        # Returns\n",
        "        decoded: refers to the output of decoder layer over the vocabulary. Note that you don't need to pass it through the softmax layer\n",
        "        hidden: stores the hidden states in LSTM\n",
        "        \"\"\"\n",
        "        z, hidden = self.lstm(self.dropout1(self.encoder(input)), hidden)\n",
        "        z = self.dropout2(z)\n",
        "        decoded = self.output(z).view(-1, self.vocab_size)\n",
        "        return decoded, hidden\n",
        "\n",
        "    # initialize parameters in LSTM\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "            weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hVpQfjuxOm0Y"
      },
      "outputs": [],
      "source": [
        "# Set the random seed for reproducibility.\n",
        "torch.manual_seed(SEED)\n",
        "# set device as GPU/CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xZT5ZAu7EX3v"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(TRAIN_BATCH_SIZE)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, BPTT)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        hidden = repackage_hidden(hidden) # Note that the main advantage here is that the hidden value is continual from the previous forward pass\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
        "            cur_loss = total_loss / LOG_INTERVAL\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // BPTT,\n",
        "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aTsdN6rLG0fc"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(TEST_BATCH_SIZE)\n",
        "\n",
        "    loss_vals = []\n",
        "    for batch, i in enumerate(range(0, data_source.size(0)-1, BPTT)):\n",
        "        data, targets = get_batch(data_source, i)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          hidden = repackage_hidden(hidden)\n",
        "          output, hidden = model(data, hidden)\n",
        "          loss = criterion(output, targets)\n",
        "          loss_vals.append(loss.item())\n",
        "\n",
        "    average_log_loss = torch.mean(torch.FloatTensor(loss_vals))\n",
        "\n",
        "    return average_log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MYuaQ5cSHxWI",
        "outputId": "5be5dc23-39d9-4e23-d1b0-d925736e86b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/  419 batches | ms/batch 192.61 | loss  7.62 | ppl  2047.39\n",
            "| epoch   1 |   200/  419 batches | ms/batch 188.93 | loss  6.82 | ppl   912.85\n",
            "| epoch   1 |   300/  419 batches | ms/batch 190.36 | loss  6.55 | ppl   701.86\n",
            "| epoch   1 |   400/  419 batches | ms/batch 191.90 | loss  6.41 | ppl   610.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 82.92s | valid loss  6.04 | valid ppl   421.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   2 |   100/  419 batches | ms/batch 194.48 | loss  6.35 | ppl   574.70\n",
            "| epoch   2 |   200/  419 batches | ms/batch 192.28 | loss  6.20 | ppl   490.98\n",
            "| epoch   2 |   300/  419 batches | ms/batch 192.53 | loss  6.13 | ppl   459.11\n",
            "| epoch   2 |   400/  419 batches | ms/batch 192.41 | loss  6.07 | ppl   434.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 83.70s | valid loss  5.78 | valid ppl   323.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   3 |   100/  419 batches | ms/batch 194.11 | loss  6.08 | ppl   436.26\n",
            "| epoch   3 |   200/  419 batches | ms/batch 192.51 | loss  5.96 | ppl   387.67\n",
            "| epoch   3 |   300/  419 batches | ms/batch 192.69 | loss  5.91 | ppl   369.54\n",
            "| epoch   3 |   400/  419 batches | ms/batch 192.41 | loss  5.87 | ppl   353.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 83.74s | valid loss  5.62 | valid ppl   275.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   4 |   100/  419 batches | ms/batch 194.17 | loss  5.89 | ppl   362.49\n",
            "| epoch   4 |   200/  419 batches | ms/batch 192.18 | loss  5.79 | ppl   328.49\n",
            "| epoch   4 |   300/  419 batches | ms/batch 192.28 | loss  5.75 | ppl   314.90\n",
            "| epoch   4 |   400/  419 batches | ms/batch 192.34 | loss  5.72 | ppl   305.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 83.63s | valid loss  5.51 | valid ppl   247.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   5 |   100/  419 batches | ms/batch 194.41 | loss  5.75 | ppl   315.07\n",
            "| epoch   5 |   200/  419 batches | ms/batch 192.64 | loss  5.67 | ppl   289.66\n",
            "| epoch   5 |   300/  419 batches | ms/batch 192.70 | loss  5.63 | ppl   279.15\n",
            "| epoch   5 |   400/  419 batches | ms/batch 192.92 | loss  5.60 | ppl   270.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 83.83s | valid loss  5.43 | valid ppl   227.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   6 |   100/  419 batches | ms/batch 194.13 | loss  5.64 | ppl   282.64\n",
            "| epoch   6 |   200/  419 batches | ms/batch 192.31 | loss  5.57 | ppl   261.64\n",
            "| epoch   6 |   300/  419 batches | ms/batch 192.38 | loss  5.53 | ppl   252.63\n",
            "| epoch   6 |   400/  419 batches | ms/batch 192.72 | loss  5.50 | ppl   244.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 83.72s | valid loss  5.36 | valid ppl   212.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   7 |   100/  419 batches | ms/batch 194.67 | loss  5.54 | ppl   255.69\n",
            "| epoch   7 |   200/  419 batches | ms/batch 192.82 | loss  5.48 | ppl   239.03\n",
            "| epoch   7 |   300/  419 batches | ms/batch 192.36 | loss  5.44 | ppl   230.41\n",
            "| epoch   7 |   400/  419 batches | ms/batch 192.42 | loss  5.41 | ppl   222.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 83.78s | valid loss  5.30 | valid ppl   201.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   8 |   100/  419 batches | ms/batch 194.47 | loss  5.45 | ppl   233.68\n",
            "| epoch   8 |   200/  419 batches | ms/batch 192.64 | loss  5.39 | ppl   218.68\n",
            "| epoch   8 |   300/  419 batches | ms/batch 192.24 | loss  5.36 | ppl   212.06\n",
            "| epoch   8 |   400/  419 batches | ms/batch 192.24 | loss  5.32 | ppl   204.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 83.69s | valid loss  5.25 | valid ppl   190.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   9 |   100/  419 batches | ms/batch 194.39 | loss  5.38 | ppl   216.52\n",
            "| epoch   9 |   200/  419 batches | ms/batch 192.30 | loss  5.32 | ppl   204.58\n",
            "| epoch   9 |   300/  419 batches | ms/batch 192.46 | loss  5.28 | ppl   196.18\n",
            "| epoch   9 |   400/  419 batches | ms/batch 192.00 | loss  5.25 | ppl   189.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 83.66s | valid loss  5.20 | valid ppl   182.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  10 |   100/  419 batches | ms/batch 193.76 | loss  5.30 | ppl   200.70\n",
            "| epoch  10 |   200/  419 batches | ms/batch 192.45 | loss  5.25 | ppl   191.23\n",
            "| epoch  10 |   300/  419 batches | ms/batch 192.33 | loss  5.21 | ppl   183.55\n",
            "| epoch  10 |   400/  419 batches | ms/batch 192.61 | loss  5.18 | ppl   177.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 83.66s | valid loss  5.17 | valid ppl   175.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  11 |   100/  419 batches | ms/batch 194.32 | loss  5.24 | ppl   188.66\n",
            "| epoch  11 |   200/  419 batches | ms/batch 192.55 | loss  5.20 | ppl   180.60\n",
            "| epoch  11 |   300/  419 batches | ms/batch 191.93 | loss  5.16 | ppl   174.29\n",
            "| epoch  11 |   400/  419 batches | ms/batch 192.32 | loss  5.12 | ppl   167.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 83.65s | valid loss  5.14 | valid ppl   170.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  12 |   100/  419 batches | ms/batch 194.23 | loss  5.19 | ppl   178.59\n",
            "| epoch  12 |   200/  419 batches | ms/batch 192.23 | loss  5.14 | ppl   171.28\n",
            "| epoch  12 |   300/  419 batches | ms/batch 192.37 | loss  5.11 | ppl   164.95\n",
            "| epoch  12 |   400/  419 batches | ms/batch 192.12 | loss  5.07 | ppl   158.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 83.66s | valid loss  5.11 | valid ppl   165.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  13 |   100/  419 batches | ms/batch 194.28 | loss  5.13 | ppl   169.82\n",
            "| epoch  13 |   200/  419 batches | ms/batch 192.23 | loss  5.09 | ppl   162.30\n",
            "| epoch  13 |   300/  419 batches | ms/batch 192.06 | loss  5.06 | ppl   157.37\n",
            "| epoch  13 |   400/  419 batches | ms/batch 192.25 | loss  5.02 | ppl   151.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 83.63s | valid loss  5.09 | valid ppl   161.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  14 |   100/  419 batches | ms/batch 194.38 | loss  5.09 | ppl   161.95\n",
            "| epoch  14 |   200/  419 batches | ms/batch 192.46 | loss  5.05 | ppl   155.90\n",
            "| epoch  14 |   300/  419 batches | ms/batch 192.40 | loss  5.01 | ppl   150.53\n",
            "| epoch  14 |   400/  419 batches | ms/batch 192.48 | loss  4.98 | ppl   145.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 83.71s | valid loss  5.07 | valid ppl   159.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  15 |   100/  419 batches | ms/batch 194.34 | loss  5.05 | ppl   155.72\n",
            "| epoch  15 |   200/  419 batches | ms/batch 192.44 | loss  5.01 | ppl   149.70\n",
            "| epoch  15 |   300/  419 batches | ms/batch 192.49 | loss  4.97 | ppl   144.38\n",
            "| epoch  15 |   400/  419 batches | ms/batch 192.42 | loss  4.94 | ppl   139.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 83.72s | valid loss  5.06 | valid ppl   157.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  16 |   100/  419 batches | ms/batch 194.21 | loss  5.01 | ppl   150.27\n",
            "| epoch  16 |   200/  419 batches | ms/batch 192.30 | loss  4.97 | ppl   144.59\n",
            "| epoch  16 |   300/  419 batches | ms/batch 192.61 | loss  4.94 | ppl   139.84\n",
            "| epoch  16 |   400/  419 batches | ms/batch 192.18 | loss  4.90 | ppl   134.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 83.68s | valid loss  5.04 | valid ppl   154.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  17 |   100/  419 batches | ms/batch 194.01 | loss  4.98 | ppl   144.91\n",
            "| epoch  17 |   200/  419 batches | ms/batch 192.49 | loss  4.94 | ppl   139.83\n",
            "| epoch  17 |   300/  419 batches | ms/batch 192.19 | loss  4.90 | ppl   134.59\n",
            "| epoch  17 |   400/  419 batches | ms/batch 192.57 | loss  4.87 | ppl   130.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 83.65s | valid loss  5.03 | valid ppl   153.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  18 |   100/  419 batches | ms/batch 194.21 | loss  4.94 | ppl   140.03\n",
            "| epoch  18 |   200/  419 batches | ms/batch 192.45 | loss  4.91 | ppl   135.90\n",
            "| epoch  18 |   300/  419 batches | ms/batch 192.34 | loss  4.87 | ppl   130.69\n",
            "| epoch  18 |   400/  419 batches | ms/batch 192.15 | loss  4.84 | ppl   126.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 83.66s | valid loss  5.02 | valid ppl   151.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  19 |   100/  419 batches | ms/batch 193.84 | loss  4.91 | ppl   135.85\n",
            "| epoch  19 |   200/  419 batches | ms/batch 192.27 | loss  4.88 | ppl   132.03\n",
            "| epoch  19 |   300/  419 batches | ms/batch 192.54 | loss  4.84 | ppl   127.08\n",
            "| epoch  19 |   400/  419 batches | ms/batch 192.05 | loss  4.81 | ppl   122.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 83.63s | valid loss  5.02 | valid ppl   150.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  20 |   100/  419 batches | ms/batch 194.00 | loss  4.88 | ppl   131.67\n",
            "| epoch  20 |   200/  419 batches | ms/batch 191.80 | loss  4.85 | ppl   128.15\n",
            "| epoch  20 |   300/  419 batches | ms/batch 192.38 | loss  4.82 | ppl   123.86\n",
            "| epoch  20 |   400/  419 batches | ms/batch 191.85 | loss  4.78 | ppl   119.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 83.54s | valid loss  5.01 | valid ppl   150.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n"
          ]
        }
      ],
      "source": [
        "# prepare the model, loss, and optimizer\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = LSTMModel(ntokens, WORD_EMBED_DIM, HID_EMBED_DIM, N_LAYERS, DROPOUT, TIED).to(device)\n",
        "criterion = nn.CrossEntropyLoss() # use crossentropy loss\n",
        "optimizer = torch.optim.Adam(model.parameters()) # use adam optimizer with default setting\n",
        "best_val_loss = None\n",
        "\n",
        "# Training framework\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "        val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # Save the model if the validation loss is the best we've seen so far.\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        with open(SAVE_BEST, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "            print(\"save new best model!\")\n",
        "        best_val_loss = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1XxYhaWUicWl",
        "outputId": "577f5439-dcec-4bee-e69f-36c1b315aa5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  4.94 | test ppl   139.71\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load the best saved model.\n",
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    model.lstm.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWSg_h9_99oZ",
        "outputId": "f0cddd85-c92e-40a6-ef29-36d2a0f4663a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# GPT-2 Text Generation\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "input_ids = tokenizer.encode('I went to', return_tensors='tf')\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq1JnD1DAj09",
        "outputId": "5f740463-6fad-40bf-8773-31b6b32b7c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top K Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to a store one weekend and someone asked me about this,\" said Ryan. \"They never went back and told me, 'Glad you asked'.\" To add insult to injury, there is a line in her life which says: \"I \n",
            "\n",
            "Top P Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to the store yesterday and bought quite a few things. I'm not sure about the price as of yet, so I'm sure one day it will be nice if it's an extra 60 dollars. Might as well check out what I've \n",
            "\n",
            "Temperature Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to bed, and as soon as I woke up had given the room to my sons, and I was quite glad to hear they came to see me.\n",
            "\n",
            "\"I had a few desires to go to my parents; but when I \n",
            "\n",
            "Beam Search Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to the doctor and said, 'I don't know what's going on. I don't know what's going on. I don't know what's going on. I don't know what's going on. I don't know what \n",
            "\n",
            "Greedy Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I went to the hospital and was told that I was going to be taken to the hospital. I was told that I was going to be taken to the hospital and that I was going to be taken to the hospital. I was told that I was \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Top K\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=50, top_k=50)\n",
        "print(\"Top K Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True), \"\\n\")\n",
        "\n",
        "# Top P\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=50, top_p=0.92, top_k=0)\n",
        "print(\"Top P Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True), \"\\n\")\n",
        "\n",
        "# Random Sampling with Temperature\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=50, top_k=0, temperature=0.7)\n",
        "print(\"Temperature Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True), \"\\n\")\n",
        "\n",
        "# Beam Search\n",
        "beam_output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "print(\"Beam Search Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True), \"\\n\")\n",
        "\n",
        "# Greedy\n",
        "greedy_output = model.generate(input_ids, max_length=50)\n",
        "print(\"Greedy Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aKmS7WvIHhwG"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, sampling_func):\n",
        "    # # Generation with LSTM lm given a sampling function and a prompt\n",
        "    max_length = 30\n",
        "    ids = []\n",
        "    for word in prompt.split():\n",
        "        ids.append(corpus.dictionary.word2idx[word])\n",
        "    hidden = model.init_hidden(1)\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        output, hidden = model(torch.LongTensor([[wid] for wid in ids]).to(device), hidden)\n",
        "        word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "        generations = []\n",
        "        for i in range(max_length):\n",
        "            word_idx = sampling_func(word_prob)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            generations.append(word)\n",
        "            if word == \"<eos>\":\n",
        "                break\n",
        "            new_word = torch.LongTensor([[word_idx]]).to(device)\n",
        "            output, hidden = model(new_word, hidden)\n",
        "            word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "    return generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MnEmUWiwHhwG"
      },
      "outputs": [],
      "source": [
        "def greedy_sampling(word_prob):\n",
        "    word_id = torch.argmax(word_prob).item()\n",
        "    return word_id\n",
        "\n",
        "def random_sampling(word_prob):\n",
        "    word_id = word_prob.multinomial(num_samples=1).item()\n",
        "    return word_id\n",
        "\n",
        "def topk_sampling(word_prob):\n",
        "    k = 10\n",
        "    top_k, idx = torch.topk(word_prob, k)\n",
        "    top_k = F.normalize(top_k, dim=0)\n",
        "    id = top_k.multinomial(num_samples=1).item()\n",
        "    word_id = idx[id].item()\n",
        "    return word_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XpCGmlnTHhwG",
        "outputId": "1a4d8b7b-bab8-4233-de0d-fa8eaac41021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt: i went to\n",
            "the first time of the game . <eos>\n",
            "\n",
            "prompt: we played with\n",
            "the <unk> <unk> ) , the <unk> of the <unk> is a <unk> , and the <unk> of the <unk> . <eos>\n",
            "\n",
            "prompt: i absolutely love\n",
            "<unk> <unk> ! \" <eos>\n",
            "\n",
            "prompt: i absolutely hate\n",
            "the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> . <eos>\n"
          ]
        }
      ],
      "source": [
        "# Greedy Sampling\n",
        "prompt = \"i went to\".lower()\n",
        "generations = generate_text(prompt, greedy_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"we played with\".lower()\n",
        "generations = generate_text(prompt, greedy_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"I absolutely love\".lower()\n",
        "generations = generate_text(prompt, greedy_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"I absolutely hate\".lower()\n",
        "generations = generate_text(prompt, greedy_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghFSQxQ8Kr4E",
        "outputId": "dc08750b-eac3-4f79-848b-78b5a07c1c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt: i went to\n",
            "runway 5 , the time in the manager among rescue problems , which has a tiny , such as the scientific chin and highest @-@ sensitive stations with a small\n",
            "\n",
            "prompt: we played with\n",
            ". in the set of 1896 wheeler was darden 's best son , which under financial genesis . <eos>\n",
            "\n",
            "prompt: i absolutely love\n",
            "beyond <unk> , <unk> <unk> ( 60 : 5 inc . long ) . <eos>\n",
            "\n",
            "prompt: i absolutely hate\n",
            "la rosa <unk> . <eos>\n"
          ]
        }
      ],
      "source": [
        "# Random Sampling\n",
        "prompt = \"i went to\".lower()\n",
        "generations = generate_text(prompt, random_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"we played with\".lower()\n",
        "generations = generate_text(prompt, random_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"I absolutely love\".lower()\n",
        "generations = generate_text(prompt, random_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"I absolutely hate\".lower()\n",
        "generations = generate_text(prompt, random_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK6qc7rXJQK_",
        "outputId": "be58eafc-40ce-479a-8050-3c0e523b5aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt: i went to\n",
            "their first time on the end of the season . in january 2013 , the team was the most famous for the first one in his first week . <eos>\n",
            "\n",
            "prompt: we played with\n",
            "the <unk> of the <unk> and the most important songs of the series , <unk> <unk> , and <unk> <unk> in which the player 's first work , which is\n",
            "\n",
            "prompt: i absolutely love\n",
            "a a <unk> <unk> in this way to the <unk> . <eos>\n",
            "\n",
            "prompt: i absolutely hate\n",
            "her <unk> . <eos>\n"
          ]
        }
      ],
      "source": [
        "# Top K Sampling\n",
        "prompt = \"i went to\".lower()\n",
        "generations = generate_text(prompt, topk_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"we played with\".lower()\n",
        "generations = generate_text(prompt, topk_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"I absolutely love\".lower()\n",
        "generations = generate_text(prompt, topk_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))\n",
        "print(\"\")\n",
        "prompt = \"I absolutely hate\".lower()\n",
        "generations = generate_text(prompt, topk_sampling)\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkGRXBf-9ZGI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
