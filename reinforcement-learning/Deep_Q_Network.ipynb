{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxzXzX8GeSOO"
      },
      "source": [
        "# Deep-Q Reinforcement Learning for Retro Video Games\n",
        "- Built off of the [OpenAI Gym framework](https://github.com/openai/mlsh/tree/master/gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzNoiGU2zy0h",
        "outputId": "9aa8ab82-6a7b-4cb7-b69b-34bfa4f33ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNP2Zg9DVEcP",
        "outputId": "dfb294d2-0fd7-478b-fac8-fbda02c8e430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.15.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 0s (3,242 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Collecting piglet\n",
            "  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n",
            "Collecting piglet-templates\n",
            "  Downloading piglet_templates-1.2.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.2.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (3.0.6)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Installing collected packages: piglet-templates, piglet\n",
            "Successfully installed piglet-1.0.0 piglet-templates-1.2.0\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.1\n",
            "  Downloading ale_py-0.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]) (5.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.62.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[accept-rom-license,atari]) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[accept-rom-license,atari]) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2021.10.8)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441026 sha256=4049ab5644732fc0bdebeafb5ec275bcc50f1f320d2d9ecc273272961a2b043e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.3 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install atari-py\n",
        "!apt-get install python-opengl -y\n",
        "!pip install piglet\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1ni0LOvU7Oj",
        "outputId": "fb8f8983-53ce-435f-a0bc-a3a69e122f85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, time, copy\n",
        "\n",
        "# Import OpenAI Gym\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR6Bgx0UVVGC",
        "outputId": "fd291ded-8c75-4506-a790-657ada4ba5ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Get GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KHZp2wRVc6B",
        "outputId": "7b313564-671f-4959-c078-7bd0f85be535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inspect environment:\n",
            "  - Shape of state: (210, 160, 3)\n",
            "  - Game info: {'lives': 4}\n"
          ]
        }
      ],
      "source": [
        "# Initialize Super Mario environment\n",
        "\n",
        "env = gym.make('ALE/Frogger-v5')\n",
        "height, width, channels = env.observation_space.shape\n",
        "actions = 5\n",
        "# env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "\n",
        "# # Limit the action-space\n",
        "# actions = [\n",
        "#   [\"right\"],        # Move right\n",
        "#   [\"right\", \"A\"]    # Jump right\n",
        "# ]\n",
        "# env = JoypadSpace(env, actions)\n",
        "\n",
        "env.reset()\n",
        "next_state, _, _, info = env.step(action=0)\n",
        "print(\n",
        "  f\"Inspect environment:\\n\"\n",
        "  f\"  - Shape of state: {next_state.shape}\\n\"\n",
        "  f\"  - Game info: {info}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQcRmZWdVekA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Classes for configuring environment and frame pre-processing for neural network.\n",
        "\n",
        "Note: \"observation\" refers to our agent's observation of their environment,\n",
        "which is given by the frames of the game in our case.\n",
        "\"\"\"\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  Implement environment step function with frame skipping.\n",
        "\n",
        "  Arguments\n",
        "    env: game environment\n",
        "    skip (int): number of frames to skip between steps\n",
        "  \"\"\"\n",
        "  def __init__(self, env, skip):\n",
        "    super().__init__(env)\n",
        "    self.skip = skip\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Repeat same action and accumulate reward SKIP times per step.\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    done = False\n",
        "    for i in range(self.skip):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "  \"\"\"\n",
        "  Perform grayscale conversion on frames input to neural network.\n",
        "\n",
        "  Arguments\n",
        "    env: game environment\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    \"\"\"\n",
        "    Set dimensions and specs for grayscale frames.\n",
        "    \"\"\"\n",
        "    super().__init__(env)\n",
        "    obs_shape = self.observation_space.shape[:2]\n",
        "    self.observation_space = Box(low=0,\n",
        "                                  high=255,\n",
        "                                  shape=obs_shape,\n",
        "                                  dtype=np.uint8)\n",
        "\n",
        "  def permute_observation(self, observation):\n",
        "    \"\"\"\n",
        "    Permute dimensions given by gym tensor (H,W,C) to expected dimensions for\n",
        "    PyTorch tensor (C,H,W).\n",
        "    \"\"\"\n",
        "    observation = np.transpose(observation, (2, 0, 1))\n",
        "    observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "    return observation\n",
        "\n",
        "  def observation(self, observation):\n",
        "    \"\"\"\n",
        "    Perform permutation and grayscale conversion on given observation.\n",
        "    \"\"\"\n",
        "    observation = self.permute_observation(observation)\n",
        "    transform = T.Grayscale()\n",
        "    observation = transform(observation)\n",
        "    return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "  \"\"\"\n",
        "  Resize (or crop) each observation to given shape.\n",
        "\n",
        "  Arguments\n",
        "    env: game environment\n",
        "    shape: (height, width) tuple specifying desired shape of observations\n",
        "  \"\"\"\n",
        "  def __init__(self, env, shape):\n",
        "    super().__init__(env)\n",
        "    self.shape = tuple(shape)\n",
        "\n",
        "    # Apply new observation shape\n",
        "    obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def observation(self, observation):\n",
        "    \"\"\"\n",
        "    Resize and normalize observations.\n",
        "    \"\"\"\n",
        "    transforms = T.Compose(\n",
        "        [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "    )\n",
        "    observation = transforms(observation).squeeze(0)\n",
        "    return observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGKBubYlhWg4"
      },
      "outputs": [],
      "source": [
        "# Apply wrappers for observation processing\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=(84,84))\n",
        "env = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G03So2NkVi-n"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "  def __init__(self, state_dim, n_actions):\n",
        "    self.state_dim = state_dim\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    # Deep Q-network to learn actions\n",
        "    self.dqn = DQN(self.state_dim, self.n_actions).float().to(device)\n",
        "\n",
        "    # Rate at which to explore new actions\n",
        "    self.epsilon = 1\n",
        "    self.epsilon_decay = 0.99999975\n",
        "    self.epsilon_min = 0.1\n",
        "    self.step_num = 0\n",
        "\n",
        "  def act(self, state):\n",
        "    \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action.\n",
        "\n",
        "    Arguments:\n",
        "      state: single observation of current game state\n",
        "\n",
        "    Returns:\n",
        "      action_idx: index of chosen action\n",
        "    \"\"\"\n",
        "    # Choose a random new action to gain experience\n",
        "    if np.random.rand() < self.epsilon:\n",
        "      action_idx = np.random.randint(self.n_actions)\n",
        "\n",
        "    # Otherwise, choose action to maximize Q function\n",
        "    else:\n",
        "      state = state.__array__()\n",
        "      state = torch.tensor(state).to(device)\n",
        "      state = state.unsqueeze(0)\n",
        "      q_values = self.dqn(state, model=\"main\")\n",
        "      action_idx = torch.argmax(q_values, axis=1).item()\n",
        "\n",
        "    # Epsilon decay\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon_min, self.epsilon)\n",
        "\n",
        "    # Increment step\n",
        "    self.step_num += 1\n",
        "    return action_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyEtTw8AVnV2"
      },
      "outputs": [],
      "source": [
        "class Agent(Agent):\n",
        "  \"\"\"\n",
        "  Define agent's memory buffer and implement memory sampling for experience\n",
        "  replay.\n",
        "\n",
        "  Arguments\n",
        "    state_dim: shape of game states\n",
        "    n_actions: number of actions in action space\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, n_actions):\n",
        "    super().__init__(state_dim, n_actions)\n",
        "    self.memory = deque(maxlen=10000)\n",
        "    self.batch_size = 8\n",
        "\n",
        "  def cache(self, state, next_state, action, reward, done):\n",
        "    \"\"\"\n",
        "    Cache an experience in agent's memory buffer.\n",
        "\n",
        "    Arguments\n",
        "      state: game state when action was taken\n",
        "      next_state: game state after action was taken\n",
        "      action: index of action taken\n",
        "      reward: reward accrued by taking action\n",
        "      done: whether transition\n",
        "    \"\"\"\n",
        "    state = state.__array__()\n",
        "    next_state = next_state.__array__()\n",
        "\n",
        "    state = torch.tensor(state).to(device)\n",
        "    next_state = torch.tensor(next_state).to(device)\n",
        "    action = torch.tensor([action]).to(device)\n",
        "    reward = torch.tensor([reward]).to(device)\n",
        "    done = torch.tensor([done]).to(device)\n",
        "\n",
        "    self.memory.append((state, next_state, action, reward, done))\n",
        "\n",
        "  def recall(self):\n",
        "    \"\"\"\n",
        "    Retrieve random batch of experiences from memory\n",
        "    \"\"\"\n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "    return state, \\\n",
        "           next_state, \\\n",
        "           action.squeeze(), \\\n",
        "           reward.squeeze(), \\\n",
        "           done.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDYdLxuCVsbl"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "  \"\"\"\n",
        "  Define convolutional Deep Q-Network architecture to choose and learn actions.\n",
        "\n",
        "  Arguments\n",
        "    state_dim: shape of input tensor given as (C, H, W)\n",
        "    n_actions: number of actions in action space\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, n_actions):\n",
        "    super().__init__()\n",
        "\n",
        "    # Main DQN\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=state_dim[0], out_channels=32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, n_actions),\n",
        "    )\n",
        "\n",
        "    # Target DQN\n",
        "    self.target = copy.deepcopy(self.net)\n",
        "\n",
        "    # Prevent target network updates\n",
        "    for p in self.target.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def forward(self, input, model):\n",
        "    \"\"\"\n",
        "    Perform forward pass for given model.\n",
        "    \"\"\"\n",
        "    if model == \"main\":\n",
        "        return self.net(input)\n",
        "    elif model == \"target\":\n",
        "        return self.target(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJjNZBFDVyor"
      },
      "outputs": [],
      "source": [
        "class Agent(Agent):\n",
        "  \"\"\"\n",
        "  Compute Q-values and target Q-values.\n",
        "\n",
        "  Arguments\n",
        "    state_dim: shape of input tensor given as (C, H, W)\n",
        "    n_actions: number of actions in action space\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, n_actions):\n",
        "    super().__init__(state_dim, n_actions)\n",
        "\n",
        "    # Set discount factor\n",
        "    self.gamma = 0.9\n",
        "\n",
        "  def estimate(self, state, action):\n",
        "    \"\"\"\n",
        "    Compute Q values over batch for given action.\n",
        "    \"\"\"\n",
        "    current_Q = self.dqn(state, model=\"main\")[\n",
        "      np.arange(0, self.batch_size), action\n",
        "    ]\n",
        "    return current_Q\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def target(self, reward, next_state, done):\n",
        "    \"\"\"\n",
        "    Compute target value labels for Q network.\n",
        "    \"\"\"\n",
        "    q_next = self.dqn(next_state, model=\"main\")\n",
        "    best_action = torch.argmax(q_next, axis=1)\n",
        "    q_next = self.dqn(next_state, model=\"target\")[\n",
        "      np.arange(0, self.batch_size), best_action\n",
        "    ]\n",
        "    return (reward + (1 - done.float()) * self.gamma * q_next).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBSQGZSYZjc_"
      },
      "outputs": [],
      "source": [
        "class Agent(Agent):\n",
        "  \"\"\"\n",
        "  Define loss and optimizer to update agents.\n",
        "\n",
        "  Arguments\n",
        "    state_dim: shape of input tensor given as (C, H, W)\n",
        "    n_actions: number of actions in action space\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, n_actions):\n",
        "    super().__init__(state_dim, n_actions)\n",
        "\n",
        "    # Adam optimizer\n",
        "    self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=0.00025)\n",
        "\n",
        "    # L1 loss for difference between target and Q-values\n",
        "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "  def update_main(self, estimate, target):\n",
        "    \"\"\"\n",
        "    Update main Q network using loss on target.\n",
        "    \"\"\"\n",
        "    loss = self.loss_fn(estimate, target)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "  def sync_target(self):\n",
        "    \"\"\"\n",
        "    Synchronize target network with main.\n",
        "    \"\"\"\n",
        "    self.dqn.target.load_state_dict(self.dqn.net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET41cbMjWJVf"
      },
      "outputs": [],
      "source": [
        "class Agent(Agent):\n",
        "  \"\"\"\n",
        "  Configure and perform learning.\n",
        "\n",
        "  Arguments\n",
        "    state_dim: shape of input tensor given as (C, H, W)\n",
        "    n_actions: number of actions in action space\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, n_actions):\n",
        "    super().__init__(state_dim, n_actions)\n",
        "\n",
        "    # Experience required before training\n",
        "    self.min_experiences = 1e4\n",
        "\n",
        "    # Number of experiences between main network updates\n",
        "    self.learn_interval = 3\n",
        "\n",
        "    # Number of experiences between syncing networks\n",
        "    self.sync_interval = 1e3\n",
        "\n",
        "  def learn(self):\n",
        "\n",
        "    # Sync target network\n",
        "    if self.step_num % self.sync_interval == 0:\n",
        "        self.sync_target()\n",
        "\n",
        "    # Wait for experience threshold\n",
        "    if self.step_num < self.min_experiences:\n",
        "        return None, None\n",
        "\n",
        "    # Check update interval\n",
        "    if self.step_num % self.learn_interval != 0:\n",
        "        return None, None\n",
        "\n",
        "    # Randomly sample experiences from memory\n",
        "    state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "    # Get Q-value estimates\n",
        "    est = self.estimate(state, action)\n",
        "\n",
        "    # Get target values\n",
        "    tgt = self.target(reward, next_state, done)\n",
        "\n",
        "    # Backpropagate loss through main network\n",
        "    loss = self.update_main(est, tgt)\n",
        "\n",
        "    return (est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6YDUSS18cZj"
      },
      "outputs": [],
      "source": [
        "class RecordEvaluation:\n",
        "  \"\"\"\n",
        "  Record and report training progress and metrics.\n",
        "  \"\"\"\n",
        "  def __init__(self, log_interval):\n",
        "    # History metrics\n",
        "    self.ep_rewards = []\n",
        "    self.ep_lengths = []\n",
        "    self.ep_avg_losses = []\n",
        "    self.ep_avg_qs = []\n",
        "\n",
        "    # Moving averages, added for every call to record()\n",
        "    self.moving_avg_ep_rewards = []\n",
        "    self.moving_avg_ep_lengths = []\n",
        "    self.moving_avg_ep_avg_losses = []\n",
        "    self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "    # Current episode metric\n",
        "    self.init_episode()\n",
        "\n",
        "    # Timing\n",
        "    self.prev_time = time.time()\n",
        "\n",
        "    self.log_interval = log_interval\n",
        "\n",
        "  def log_step(self, reward, loss, q):\n",
        "    self.curr_ep_reward += reward\n",
        "    self.curr_ep_length += 1\n",
        "    if loss:\n",
        "        self.curr_ep_loss += loss\n",
        "        self.curr_ep_q += q\n",
        "        self.curr_ep_loss_length += 1\n",
        "\n",
        "  def log_episode(self):\n",
        "    self.ep_rewards.append(self.curr_ep_reward)\n",
        "    self.ep_lengths.append(self.curr_ep_length)\n",
        "    if self.curr_ep_loss_length == 0:\n",
        "        ep_avg_loss = 0\n",
        "        ep_avg_q = 0\n",
        "    else:\n",
        "        ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "        ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "    self.ep_avg_losses.append(ep_avg_loss)\n",
        "    self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "    self.init_episode()\n",
        "\n",
        "  def init_episode(self):\n",
        "    self.curr_ep_reward = 0.0\n",
        "    self.curr_ep_length = 0\n",
        "    self.curr_ep_loss = 0.0\n",
        "    self.curr_ep_q = 0.0\n",
        "    self.curr_ep_loss_length = 0\n",
        "\n",
        "  def record(self, episode, epsilon, step):\n",
        "    mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "    mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "    mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "    mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "    self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "    self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "    self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "    self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "    elapsed_time = np.round(time.time() - self.prev_time, 3)\n",
        "    self.prev_time = time.time()\n",
        "\n",
        "    print(\n",
        "      f\"Summary for episodes {episode+2-self.log_interval} through {episode+1}:\\n\"\n",
        "      f\"  - Step {step}\\n\"\n",
        "      f\"  - Epsilon {epsilon}\\n\"\n",
        "      f\"  - Mean Reward {mean_ep_reward}\\n\"\n",
        "      f\"  - Mean Length {mean_ep_length}\\n\"\n",
        "      f\"  - Mean Loss {mean_ep_loss}\\n\"\n",
        "      f\"  - Mean Q Value {mean_ep_q}\\n\"\n",
        "      f\"  - Total time (s): {elapsed_time}\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3J8kH3ie4zv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "path = '/content/drive/MyDrive/weights/agent.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LER4Pwk-WRDz",
        "outputId": "63c23b85-110c-48fd-85b2-a5b12265d504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded agent\n",
            "0.6352040095599248\n",
            "Summary for episodes 23001 through 23200:\n",
            "  - Step 14807\n",
            "  - Epsilon 0.6328569895445961\n",
            "  - Mean Reward 10.95\n",
            "  - Mean Length 73.34\n",
            "  - Mean Loss 0.006\n",
            "  - Mean Q Value 2.212\n",
            "  - Total time (s): 96.478\n",
            "\n",
            "Summary for episodes 23201 through 23400:\n",
            "  - Step 29830\n",
            "  - Epsilon 0.6304845944593359\n",
            "  - Mean Reward 11.16\n",
            "  - Mean Length 76.41\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.33\n",
            "  - Total time (s): 128.113\n",
            "\n",
            "Summary for episodes 23401 through 23600:\n",
            "  - Step 44643\n",
            "  - Epsilon 0.6281540700140065\n",
            "  - Mean Reward 11.14\n",
            "  - Mean Length 74.38\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.267\n",
            "  - Total time (s): 126.463\n",
            "\n",
            "Summary for episodes 23601 through 23800:\n",
            "  - Step 59834\n",
            "  - Epsilon 0.6257730217714323\n",
            "  - Mean Reward 10.97\n",
            "  - Mean Length 73.74\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.234\n",
            "  - Total time (s): 129.439\n",
            "\n",
            "Saving Model at Episode 23999\n",
            "Summary for episodes 23801 through 24000:\n",
            "  - Step 74380\n",
            "  - Epsilon 0.6235015305356553\n",
            "  - Mean Reward 10.37\n",
            "  - Mean Length 70.59\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.211\n",
            "  - Total time (s): 124.134\n",
            "\n",
            "Summary for episodes 24001 through 24200:\n",
            "  - Step 89145\n",
            "  - Epsilon 0.6212042727084719\n",
            "  - Mean Reward 10.67\n",
            "  - Mean Length 73.12\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.287\n",
            "  - Total time (s): 127.378\n",
            "\n",
            "Summary for episodes 24201 through 24400:\n",
            "  - Step 103816\n",
            "  - Epsilon 0.6189300236905689\n",
            "  - Mean Reward 10.39\n",
            "  - Mean Length 72.62\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.382\n",
            "  - Total time (s): 126.013\n",
            "\n",
            "Summary for episodes 24401 through 24600:\n",
            "  - Step 118722\n",
            "  - Epsilon 0.6166278728159404\n",
            "  - Mean Reward 10.94\n",
            "  - Mean Length 73.66\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.396\n",
            "  - Total time (s): 127.426\n",
            "\n",
            "Summary for episodes 24601 through 24800:\n",
            "  - Step 133821\n",
            "  - Epsilon 0.6143046440248263\n",
            "  - Mean Reward 10.99\n",
            "  - Mean Length 75.84\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.406\n",
            "  - Total time (s): 129.626\n",
            "\n",
            "Saving Model at Episode 24999\n",
            "Summary for episodes 24801 through 25000:\n",
            "  - Step 149140\n",
            "  - Epsilon 0.611956509774099\n",
            "  - Mean Reward 11.53\n",
            "  - Mean Length 76.3\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.339\n",
            "  - Total time (s): 131.325\n",
            "\n",
            "Summary for episodes 25001 through 25200:\n",
            "  - Step 164023\n",
            "  - Epsilon 0.6096838030147347\n",
            "  - Mean Reward 11.23\n",
            "  - Mean Length 74.9\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.254\n",
            "  - Total time (s): 126.351\n",
            "\n",
            "Summary for episodes 25201 through 25400:\n",
            "  - Step 178943\n",
            "  - Epsilon 0.6074139181125103\n",
            "  - Mean Reward 11.64\n",
            "  - Mean Length 76.41\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.295\n",
            "  - Total time (s): 128.12\n",
            "\n",
            "Summary for episodes 25401 through 25600:\n",
            "  - Step 193904\n",
            "  - Epsilon 0.605146281329347\n",
            "  - Mean Reward 11.25\n",
            "  - Mean Length 74.34\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.331\n",
            "  - Total time (s): 128.788\n",
            "\n",
            "Summary for episodes 25601 through 25800:\n",
            "  - Step 208646\n",
            "  - Epsilon 0.6029201192109871\n",
            "  - Mean Reward 11.39\n",
            "  - Mean Length 73.28\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.305\n",
            "  - Total time (s): 126.744\n",
            "\n",
            "Saving Model at Episode 25999\n",
            "Summary for episodes 25801 through 26000:\n",
            "  - Step 223398\n",
            "  - Epsilon 0.6007006447601363\n",
            "  - Mean Reward 11.08\n",
            "  - Mean Length 72.87\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.29\n",
            "  - Total time (s): 127.45\n",
            "\n",
            "Summary for episodes 26001 through 26200:\n",
            "  - Step 238137\n",
            "  - Epsilon 0.5984912857504575\n",
            "  - Mean Reward 11.8\n",
            "  - Mean Length 73.59\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.277\n",
            "  - Total time (s): 125.376\n",
            "\n",
            "Summary for episodes 26201 through 26400:\n",
            "  - Step 253284\n",
            "  - Epsilon 0.5962292342059594\n",
            "  - Mean Reward 12.03\n",
            "  - Mean Length 76.7\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.382\n",
            "  - Total time (s): 129.245\n",
            "\n",
            "Summary for episodes 26401 through 26600:\n",
            "  - Step 268016\n",
            "  - Epsilon 0.5940373604751991\n",
            "  - Mean Reward 11.24\n",
            "  - Mean Length 74.0\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.426\n",
            "  - Total time (s): 125.566\n",
            "\n",
            "Summary for episodes 26601 through 26800:\n",
            "  - Step 282677\n",
            "  - Epsilon 0.5918640500637697\n",
            "  - Mean Reward 11.11\n",
            "  - Mean Length 73.75\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.277\n",
            "  - Total time (s): 125.164\n",
            "\n",
            "Saving Model at Episode 26999\n",
            "Summary for episodes 26801 through 27000:\n",
            "  - Step 297856\n",
            "  - Epsilon 0.5896223297534285\n",
            "  - Mean Reward 11.86\n",
            "  - Mean Length 76.31\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.307\n",
            "  - Total time (s): 129.535\n",
            "\n",
            "Summary for episodes 27001 through 27200:\n",
            "  - Step 312751\n",
            "  - Epsilon 0.5874308062040324\n",
            "  - Mean Reward 12.6\n",
            "  - Mean Length 77.12\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.286\n",
            "  - Total time (s): 127.105\n",
            "\n",
            "Summary for episodes 27201 through 27400:\n",
            "  - Step 328024\n",
            "  - Epsilon 0.5851921248903702\n",
            "  - Mean Reward 11.9\n",
            "  - Mean Length 76.17\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.42\n",
            "  - Total time (s): 129.977\n",
            "\n",
            "Summary for episodes 27401 through 27600:\n",
            "  - Step 342950\n",
            "  - Epsilon 0.5830125492758463\n",
            "  - Mean Reward 11.95\n",
            "  - Mean Length 74.16\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.406\n",
            "  - Total time (s): 127.044\n",
            "\n",
            "Summary for episodes 27601 through 27800:\n",
            "  - Step 357780\n",
            "  - Epsilon 0.5808550319485107\n",
            "  - Mean Reward 11.79\n",
            "  - Mean Length 73.08\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.356\n",
            "  - Total time (s): 126.5\n",
            "\n",
            "Saving Model at Episode 27999\n",
            "Summary for episodes 27801 through 28000:\n",
            "  - Step 372761\n",
            "  - Epsilon 0.578683653084864\n",
            "  - Mean Reward 12.28\n",
            "  - Mean Length 75.14\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.285\n",
            "  - Total time (s): 126.774\n",
            "\n",
            "Summary for episodes 28001 through 28200:\n",
            "  - Step 387558\n",
            "  - Epsilon 0.57654691192035\n",
            "  - Mean Reward 11.35\n",
            "  - Mean Length 74.11\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.363\n",
            "  - Total time (s): 126.478\n",
            "\n",
            "Summary for episodes 28201 through 28400:\n",
            "  - Step 402343\n",
            "  - Epsilon 0.574419783751508\n",
            "  - Mean Reward 11.12\n",
            "  - Mean Length 72.7\n",
            "  - Mean Loss 0.008\n",
            "  - Mean Q Value 3.415\n",
            "  - Total time (s): 126.986\n",
            "\n",
            "Summary for episodes 28401 through 28600:\n",
            "  - Step 417199\n",
            "  - Epsilon 0.5722903452232796\n",
            "  - Mean Reward 11.81\n",
            "  - Mean Length 73.77\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.315\n",
            "  - Total time (s): 127.635\n",
            "\n",
            "Summary for episodes 28601 through 28800:\n",
            "  - Step 432182\n",
            "  - Epsilon 0.5701506981880639\n",
            "  - Mean Reward 11.38\n",
            "  - Mean Length 72.51\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.343\n",
            "  - Total time (s): 128.69\n",
            "\n",
            "Saving Model at Episode 28999\n",
            "Summary for episodes 28801 through 29000:\n",
            "  - Step 446917\n",
            "  - Epsilon 0.5680542690224498\n",
            "  - Mean Reward 11.59\n",
            "  - Mean Length 71.97\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.387\n",
            "  - Total time (s): 126.689\n",
            "\n",
            "Summary for episodes 29001 through 29200:\n",
            "  - Step 462083\n",
            "  - Epsilon 0.5659045688617611\n",
            "  - Mean Reward 12.67\n",
            "  - Mean Length 75.82\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.402\n",
            "  - Total time (s): 130.371\n",
            "\n",
            "Summary for episodes 29201 through 29400:\n",
            "  - Step 477112\n",
            "  - Epsilon 0.5637823130742479\n",
            "  - Mean Reward 12.81\n",
            "  - Mean Length 76.08\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.378\n",
            "  - Total time (s): 129.083\n",
            "\n",
            "Summary for episodes 29401 through 29600:\n",
            "  - Step 491602\n",
            "  - Epsilon 0.5617437060463132\n",
            "  - Mean Reward 11.83\n",
            "  - Mean Length 71.98\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.337\n",
            "  - Total time (s): 124.634\n",
            "\n",
            "Summary for episodes 29601 through 29800:\n",
            "  - Step 506674\n",
            "  - Epsilon 0.559631038263258\n",
            "  - Mean Reward 12.58\n",
            "  - Mean Length 76.21\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.383\n",
            "  - Total time (s): 129.367\n",
            "\n",
            "Saving Model at Episode 29999\n",
            "Summary for episodes 29801 through 30000:\n",
            "  - Step 521713\n",
            "  - Epsilon 0.5575309156462391\n",
            "  - Mean Reward 12.02\n",
            "  - Mean Length 75.48\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.32\n",
            "  - Total time (s): 129.211\n",
            "\n",
            "Summary for episodes 30001 through 30200:\n",
            "  - Step 536684\n",
            "  - Epsilon 0.555448116680323\n",
            "  - Mean Reward 11.81\n",
            "  - Mean Length 73.25\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.433\n",
            "  - Total time (s): 129.109\n",
            "\n",
            "Summary for episodes 30201 through 30400:\n",
            "  - Step 551601\n",
            "  - Epsilon 0.5533805691294731\n",
            "  - Mean Reward 11.77\n",
            "  - Mean Length 71.9\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.327\n",
            "  - Total time (s): 128.318\n",
            "\n",
            "Summary for episodes 30401 through 30600:\n",
            "  - Step 566551\n",
            "  - Epsilon 0.5513161692444192\n",
            "  - Mean Reward 12.73\n",
            "  - Mean Length 75.79\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.34\n",
            "  - Total time (s): 128.986\n",
            "\n",
            "Summary for episodes 30601 through 30800:\n",
            "  - Step 581736\n",
            "  - Epsilon 0.5492272026040803\n",
            "  - Mean Reward 12.99\n",
            "  - Mean Length 75.88\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.379\n",
            "  - Total time (s): 130.88\n",
            "\n",
            "Saving Model at Episode 30999\n",
            "Summary for episodes 30801 through 31000:\n",
            "  - Step 596965\n",
            "  - Epsilon 0.5471401325954713\n",
            "  - Mean Reward 11.93\n",
            "  - Mean Length 73.34\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.43\n",
            "  - Total time (s): 130.545\n",
            "\n",
            "Summary for episodes 31001 through 31200:\n",
            "  - Step 611893\n",
            "  - Epsilon 0.5451020108662716\n",
            "  - Mean Reward 12.54\n",
            "  - Mean Length 73.98\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.38\n",
            "  - Total time (s): 128.871\n",
            "\n",
            "Summary for episodes 31201 through 31400:\n",
            "  - Step 626790\n",
            "  - Epsilon 0.5430756900534705\n",
            "  - Mean Reward 12.42\n",
            "  - Mean Length 74.36\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.383\n",
            "  - Total time (s): 128.701\n",
            "\n",
            "Summary for episodes 31401 through 31600:\n",
            "  - Step 641742\n",
            "  - Epsilon 0.5410494622495642\n",
            "  - Mean Reward 12.32\n",
            "  - Mean Length 75.11\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.364\n",
            "  - Total time (s): 128.879\n",
            "\n",
            "Summary for episodes 31601 through 31800:\n",
            "  - Step 656807\n",
            "  - Epsilon 0.5390155669411241\n",
            "  - Mean Reward 12.87\n",
            "  - Mean Length 75.81\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.346\n",
            "  - Total time (s): 130.123\n",
            "\n",
            "Saving Model at Episode 31999\n",
            "Summary for episodes 31801 through 32000:\n",
            "  - Step 671837\n",
            "  - Epsilon 0.5369940160625604\n",
            "  - Mean Reward 12.98\n",
            "  - Mean Length 75.28\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.353\n",
            "  - Total time (s): 129.953\n",
            "\n",
            "Summary for episodes 32001 through 32200:\n",
            "  - Step 686642\n",
            "  - Epsilon 0.5350101403870682\n",
            "  - Mean Reward 13.18\n",
            "  - Mean Length 75.85\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.401\n",
            "  - Total time (s): 128.089\n",
            "\n",
            "Summary for episodes 32201 through 32400:\n",
            "  - Step 701574\n",
            "  - Epsilon 0.5330166704102189\n",
            "  - Mean Reward 12.68\n",
            "  - Mean Length 74.88\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.425\n",
            "  - Total time (s): 128.631\n",
            "\n",
            "Summary for episodes 32401 through 32600:\n",
            "  - Step 716550\n",
            "  - Epsilon 0.5310247868796769\n",
            "  - Mean Reward 12.43\n",
            "  - Mean Length 74.04\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.341\n",
            "  - Total time (s): 128.614\n",
            "\n",
            "Summary for episodes 32601 through 32800:\n",
            "  - Step 731472\n",
            "  - Epsilon 0.5290474891125561\n",
            "  - Mean Reward 13.16\n",
            "  - Mean Length 75.01\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.353\n",
            "  - Total time (s): 128.854\n",
            "\n",
            "Saving Model at Episode 32999\n",
            "Summary for episodes 32801 through 33000:\n",
            "  - Step 746615\n",
            "  - Epsilon 0.527048433680066\n",
            "  - Mean Reward 13.0\n",
            "  - Mean Length 76.09\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.445\n",
            "  - Total time (s): 130.886\n",
            "\n",
            "Summary for episodes 33001 through 33200:\n",
            "  - Step 761760\n",
            "  - Epsilon 0.5250566693371089\n",
            "  - Mean Reward 12.86\n",
            "  - Mean Length 75.52\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.404\n",
            "  - Total time (s): 130.711\n",
            "\n",
            "Summary for episodes 33201 through 33400:\n",
            "  - Step 776638\n",
            "  - Epsilon 0.5231073483087098\n",
            "  - Mean Reward 12.54\n",
            "  - Mean Length 73.76\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.374\n",
            "  - Total time (s): 128.415\n",
            "\n",
            "Summary for episodes 33401 through 33600:\n",
            "  - Step 791485\n",
            "  - Epsilon 0.5211693033603193\n",
            "  - Mean Reward 12.7\n",
            "  - Mean Length 74.07\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.396\n",
            "  - Total time (s): 128.245\n",
            "\n",
            "Summary for episodes 33601 through 33800:\n",
            "  - Step 806784\n",
            "  - Epsilon 0.5191797679814039\n",
            "  - Mean Reward 12.55\n",
            "  - Mean Length 75.89\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.363\n",
            "  - Total time (s): 132.233\n",
            "\n",
            "Saving Model at Episode 33999\n",
            "Summary for episodes 33801 through 34000:\n",
            "  - Step 822238\n",
            "  - Epsilon 0.5171777865152094\n",
            "  - Mean Reward 13.07\n",
            "  - Mean Length 76.85\n",
            "  - Mean Loss 0.012\n",
            "  - Mean Q Value 3.425\n",
            "  - Total time (s): 133.851\n",
            "\n",
            "Summary for episodes 34001 through 34200:\n",
            "  - Step 837298\n",
            "  - Epsilon 0.5152342728660199\n",
            "  - Mean Reward 13.32\n",
            "  - Mean Length 75.04\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.399\n",
            "  - Total time (s): 129.329\n",
            "\n",
            "Summary for episodes 34201 through 34400:\n",
            "  - Step 852467\n",
            "  - Epsilon 0.5132840756056521\n",
            "  - Mean Reward 13.13\n",
            "  - Mean Length 75.08\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.373\n",
            "  - Total time (s): 130.487\n",
            "\n",
            "Summary for episodes 34401 through 34600:\n",
            "  - Step 867830\n",
            "  - Epsilon 0.5113164605236046\n",
            "  - Mean Reward 13.77\n",
            "  - Mean Length 76.84\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.433\n",
            "  - Total time (s): 132.35\n",
            "\n",
            "Summary for episodes 34601 through 34800:\n",
            "  - Step 883061\n",
            "  - Epsilon 0.509373197106446\n",
            "  - Mean Reward 13.59\n",
            "  - Mean Length 76.83\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.442\n",
            "  - Total time (s): 130.74\n",
            "\n",
            "Saving Model at Episode 34999\n",
            "Summary for episodes 34801 through 35000:\n",
            "  - Step 897939\n",
            "  - Epsilon 0.5074821024067484\n",
            "  - Mean Reward 13.02\n",
            "  - Mean Length 73.89\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.41\n",
            "  - Total time (s): 127.89\n",
            "\n",
            "Summary for episodes 35001 through 35200:\n",
            "  - Step 913103\n",
            "  - Epsilon 0.5055618795982956\n",
            "  - Mean Reward 13.36\n",
            "  - Mean Length 75.67\n",
            "  - Mean Loss 0.012\n",
            "  - Mean Q Value 3.358\n",
            "  - Total time (s): 130.477\n",
            "\n",
            "Summary for episodes 35201 through 35400:\n",
            "  - Step 928247\n",
            "  - Epsilon 0.5036514408256484\n",
            "  - Mean Reward 13.7\n",
            "  - Mean Length 75.17\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.397\n",
            "  - Total time (s): 130.487\n",
            "\n",
            "Summary for episodes 35401 through 35600:\n",
            "  - Step 943195\n",
            "  - Epsilon 0.501772807568713\n",
            "  - Mean Reward 13.25\n",
            "  - Mean Length 75.85\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.387\n",
            "  - Total time (s): 128.729\n",
            "\n",
            "Summary for episodes 35601 through 35800:\n",
            "  - Step 958111\n",
            "  - Epsilon 0.49990518088969704\n",
            "  - Mean Reward 13.12\n",
            "  - Mean Length 75.32\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.368\n",
            "  - Total time (s): 128.273\n",
            "\n",
            "Saving Model at Episode 35999\n",
            "Summary for episodes 35801 through 36000:\n",
            "  - Step 973568\n",
            "  - Epsilon 0.4979771496497111\n",
            "  - Mean Reward 14.1\n",
            "  - Mean Length 77.28\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.373\n",
            "  - Total time (s): 132.883\n",
            "\n",
            "Summary for episodes 36001 through 36200:\n",
            "  - Step 988873\n",
            "  - Epsilon 0.49607540493838886\n",
            "  - Mean Reward 14.38\n",
            "  - Mean Length 79.29\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.405\n",
            "  - Total time (s): 131.055\n",
            "\n",
            "Summary for episodes 36201 through 36400:\n",
            "  - Step 1004181\n",
            "  - Epsilon 0.4941805522398627\n",
            "  - Mean Reward 14.06\n",
            "  - Mean Length 77.03\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.418\n",
            "  - Total time (s): 130.822\n",
            "\n",
            "Summary for episodes 36401 through 36600:\n",
            "  - Step 1019309\n",
            "  - Epsilon 0.49231509096866943\n",
            "  - Mean Reward 13.26\n",
            "  - Mean Length 75.97\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.392\n",
            "  - Total time (s): 128.874\n",
            "\n",
            "Summary for episodes 36601 through 36800:\n",
            "  - Step 1034618\n",
            "  - Epsilon 0.49043447888329794\n",
            "  - Mean Reward 14.04\n",
            "  - Mean Length 76.4\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.332\n",
            "  - Total time (s): 130.995\n",
            "\n",
            "Saving Model at Episode 36999\n",
            "Summary for episodes 36801 through 37000:\n",
            "  - Step 1049810\n",
            "  - Epsilon 0.4885753412370452\n",
            "  - Mean Reward 14.51\n",
            "  - Mean Length 77.11\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.364\n",
            "  - Total time (s): 129.891\n",
            "\n",
            "Summary for episodes 37001 through 37200:\n",
            "  - Step 1064878\n",
            "  - Epsilon 0.48673833986111165\n",
            "  - Mean Reward 13.49\n",
            "  - Mean Length 74.42\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.456\n",
            "  - Total time (s): 129.48\n",
            "\n",
            "Summary for episodes 37201 through 37400:\n",
            "  - Step 1080276\n",
            "  - Epsilon 0.48486824216768515\n",
            "  - Mean Reward 13.88\n",
            "  - Mean Length 77.31\n",
            "  - Mean Loss 0.012\n",
            "  - Mean Q Value 3.393\n",
            "  - Total time (s): 132.049\n",
            "\n",
            "Summary for episodes 37401 through 37600:\n",
            "  - Step 1095774\n",
            "  - Epsilon 0.4829932545940191\n",
            "  - Mean Reward 13.84\n",
            "  - Mean Length 76.1\n",
            "  - Mean Loss 0.012\n",
            "  - Mean Q Value 3.388\n",
            "  - Total time (s): 132.668\n",
            "\n",
            "Summary for episodes 37601 through 37800:\n",
            "  - Step 1111016\n",
            "  - Epsilon 0.4811563106276976\n",
            "  - Mean Reward 14.41\n",
            "  - Mean Length 76.73\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.377\n",
            "  - Total time (s): 130.427\n",
            "\n",
            "Saving Model at Episode 37999\n",
            "Summary for episodes 37801 through 38000:\n",
            "  - Step 1126451\n",
            "  - Epsilon 0.47930322607653675\n",
            "  - Mean Reward 14.34\n",
            "  - Mean Length 76.99\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.365\n",
            "  - Total time (s): 132.299\n",
            "\n",
            "Summary for episodes 38001 through 38200:\n",
            "  - Step 1141629\n",
            "  - Epsilon 0.4774879559573172\n",
            "  - Mean Reward 14.14\n",
            "  - Mean Length 75.88\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.348\n",
            "  - Total time (s): 130.091\n",
            "\n",
            "Summary for episodes 38201 through 38400:\n",
            "  - Step 1156994\n",
            "  - Epsilon 0.47565732332715316\n",
            "  - Mean Reward 14.22\n",
            "  - Mean Length 76.79\n",
            "  - Mean Loss 0.012\n",
            "  - Mean Q Value 3.409\n",
            "  - Total time (s): 131.778\n",
            "\n",
            "Summary for episodes 38401 through 38600:\n",
            "  - Step 1172398\n",
            "  - Epsilon 0.4738290892703334\n",
            "  - Mean Reward 14.31\n",
            "  - Mean Length 76.67\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.351\n",
            "  - Total time (s): 132.134\n",
            "\n",
            "Summary for episodes 38601 through 38800:\n",
            "  - Step 1187609\n",
            "  - Epsilon 0.47203065713719994\n",
            "  - Mean Reward 13.49\n",
            "  - Mean Length 74.41\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.319\n",
            "  - Total time (s): 130.366\n",
            "\n",
            "Saving Model at Episode 38999\n",
            "Summary for episodes 38801 through 39000:\n",
            "  - Step 1203088\n",
            "  - Epsilon 0.470207546040766\n",
            "  - Mean Reward 14.81\n",
            "  - Mean Length 77.32\n",
            "  - Mean Loss 0.009\n",
            "  - Mean Q Value 3.454\n",
            "  - Total time (s): 133.24\n",
            "\n",
            "Summary for episodes 39001 through 39200:\n",
            "  - Step 1218434\n",
            "  - Epsilon 0.4684070505739165\n",
            "  - Mean Reward 14.35\n",
            "  - Mean Length 76.8\n",
            "  - Mean Loss 0.01\n",
            "  - Mean Q Value 3.452\n",
            "  - Total time (s): 131.46\n",
            "\n",
            "Summary for episodes 39201 through 39400:\n",
            "  - Step 1234065\n",
            "  - Epsilon 0.46658020444714243\n",
            "  - Mean Reward 15.12\n",
            "  - Mean Length 79.51\n",
            "  - Mean Loss 0.011\n",
            "  - Mean Q Value 3.455\n",
            "  - Total time (s): 134.4\n",
            "\n",
            "Summary for episodes 39401 through 39600:\n",
            "  - Step 1249345\n",
            "  - Epsilon 0.46480126777582514\n",
            "  - Mean Reward 14.16\n",
            "  - Mean Length 76.68\n",
            "  - Mean Loss 0.016\n",
            "  - Mean Q Value 3.352\n",
            "  - Total time (s): 131.094\n",
            "\n"
          ]
        }
      ],
      "source": [
        "agent = Agent(state_dim=(4, 84, 84), n_actions=env.action_space.n)\n",
        "# agent.step_num = 692632\n",
        "# episodes = 0\n",
        "if os.path.getsize(path) != 0:\n",
        "  checkpoint = torch.load(path)\n",
        "  agent.dqn.net.load_state_dict(checkpoint['model_state_dict'])\n",
        "  agent.dqn.target.load_state_dict(checkpoint['target_state_dict'])\n",
        "  # episodes = checkpoint['episode']\n",
        "  agent.epsilon = checkpoint['epsilon']\n",
        "  print(\"Loaded agent\")\n",
        "\n",
        "print(agent.epsilon)\n",
        "episodes = 23000\n",
        "record = RecordEvaluation(log_interval=200)\n",
        "save_interval = 1000\n",
        "for e in range(episodes, episodes+20000):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "        # Run agent on the state\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        agent.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = agent.learn()\n",
        "\n",
        "        # Logging\n",
        "        record.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or (info['lives'] == 0):\n",
        "            break\n",
        "\n",
        "    record.log_episode()\n",
        "\n",
        "    if (e+1) % save_interval == 0:\n",
        "      print(\"Saving Model at Episode\", e)\n",
        "      torch.save({'epsilon': agent.epsilon, 'steps':agent.step_num, 'episode': e, 'model_state_dict': agent.dqn.net.state_dict(), 'target_state_dict': agent.dqn.target.state_dict()}, path)\n",
        "      df = pd.DataFrame(np.stack((record.ep_rewards, record.ep_lengths), axis=1), columns=['Reward','Length'])\n",
        "      df.to_csv(path + str(e))\n",
        "\n",
        "    if (e+1) % record.log_interval == 0:\n",
        "      record.record(episode=e, epsilon=agent.epsilon, step=agent.step_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z6rVQPu5n16"
      },
      "outputs": [],
      "source": [
        "episodes = 25\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "\n",
        "        # env.render()\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        agent.cache(state, next_state, action, reward, done)\n",
        "        q, loss = agent.learn()\n",
        "        record.log_step(reward, loss, q)\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or (info[\"lives\"] == 0):\n",
        "            break\n",
        "\n",
        "    record.log_episode()\n",
        "    record.record(episode=e, epsilon=agent.epsilon, step=agent.step_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCK6TGZwSbiD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
